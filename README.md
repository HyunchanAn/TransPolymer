[í•œêµ­ì–´ ë²„ì „ì€ ì•„ë˜ì— ìˆìœ¼ë©°, ì˜ì–´ ì›ë¬¸ì€ ë¬¸ì„œ í•˜ë‹¨ì— ë³´ì¡´ë˜ì–´ ìˆìŠµë‹ˆë‹¤.]
[The Korean version is below, and the original English version is preserved at the bottom of the document.]

---

## TransPolymer (íŠ¸ëœìŠ¤í´ë¦¬ë¨¸) ##

#### npj Computational Materials [[ë…¼ë¬¸]](https://www.nature.com/articles/s41524-023-01016-5) [[arXiv]](https://arxiv.org/abs/2209.01307) [[PDF]](https://www.nature.com/articles/s41524-023-01016-5.pdf) </br>
[Changwen Xu](https://changwenxu98.github.io/), [Yuyang Wang](https://yuyangw.github.io/), [Amir Barati Farimani](https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html) </br>
ì¹´ë„¤ê¸° ë©œëŸ° ëŒ€í•™êµ (Carnegie Mellon University) </br>

<img src="figs/pipeline.png" width="500">

ì´ ë ˆí¬ì§€í† ë¦¬ëŠ” <strong><em>TransPolymer</em></strong>ì˜ ê³µì‹ êµ¬í˜„ì²´ì…ë‹ˆë‹¤: ["TransPolymer: a Transformer-based language model for polymer property predictions"](https://www.nature.com/articles/s41524-023-01016-5). ì´ ì—°êµ¬ì—ì„œëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬, ë¼ë²¨ì´ ì—†ëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì…‹(ì•½ 500ë§Œ ê°œì˜ ê³ ë¶„ì ì‹œë‚˜ë¦¬ì˜¤)ì— ëŒ€í•´ ìê¸° ì§€ë„ í•™ìŠµ(Masked Language Modeling)ì„ ìˆ˜í–‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ìœ„ ì‘ì—…(Downstream tasks)ì—ì„œ ê³ ë¶„ì ë¬¼ì„±ì„ ì •í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì—°êµ¬ì— ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ ì•„ë˜ë¥¼ ì¸ìš©í•´ ì£¼ì„¸ìš”:

```
@article{xu2023transpolymer,
  title={TransPolymer: a Transformer-based language model for polymer property predictions},
  author={Xu, Changwen and Wang, Yuyang and Barati Farimani, Amir},
  journal={npj Computational Materials},
  volume={9},
  number={1},
  pages={64},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
```

> [!TIP]
> **MacOS (Apple Silicon) ì§€ì› ì•ˆë‚´:**
> ë³¸ í”„ë¡œì íŠ¸ëŠ” M1/M2/M3 ë“± Apple Silicon ì¹©ì…‹ì˜ **MPS (Metal Performance Shaders)** ê°€ì†ì„ ì™„ë²½í•˜ê²Œ ì§€ì›í•©ë‹ˆë‹¤. ë³„ë„ì˜ ì„¤ì • ì—†ì´ ìë™ìœ¼ë¡œ GPUë¥¼ ê°ì§€í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.

## ì‹œì‘í•˜ê¸°

### ë¹ ë¥¸ ì‹œì‘ (MacOS/Apple Silicon ë°ëª¨ìš©)
  
  ì´ˆê¸° í™˜ê²½ ì„¤ì •ì„ ìœ„í•´ **ì›í´ë¦­ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
  í„°ë¯¸ë„ì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ íŒŒì´ì¬ ê°€ìƒí™˜ê²½ ìƒì„± ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ê°€ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.
  
  ```bash
  bash setup_mac.sh
  ```
  
  ì„¤ì¹˜ ì™„ë£Œ í›„:
  ```bash

  # ê°€ìƒí™˜ê²½ ìƒì„± 
  bash setup_mac.sh

  # ê°€ìƒí™˜ê²½ í™œì„±í™”
  source .venv/bin/activate
  
  # ëª¨ë¸ í•™ìŠµ ì‹œì—°
  python Pretrain.py --config configs/config.yaml
  
  # íŒŒì¸íŠœë‹ (ê¸°ë³¸)
  bash run_finetune.sh
  
  # Tg-Boost (ì‹¬í™” í•™ìŠµ)
  bash run_tg_boost.sh

  # ë°ëª¨ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¼ë¦¿ ì•±)
  streamlit run app.py
  
  ```
  
  ### ì„¤ì¹˜ ë°©ë²• (ìˆ˜ë™)

Conda í™˜ê²½ì„ ì„¤ì •í•˜ê³  github ë ˆí¬ì§€í† ë¦¬ë¥¼ í´ë¡ í•©ë‹ˆë‹¤.

```bash
# ìƒˆë¡œìš´ í™˜ê²½ ìƒì„±
$ conda create --name TransPolymer python=3.9
$ conda activate TransPolymer

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
$ conda install pytorch==1.12.0 torchvision torchaudio cudatoolkit=11.3 -c pytorch
$ pip install transformers==4.20.1
$ pip install PyYAML==6.0
$ pip install fairscale==0.4.6
$ conda install -c conda-forge rdkit=2022.3.4
$ conda install -c conda-forge scikit-learn==0.24.2
$ conda install -c conda-forge tensorboard==2.9.1
$ conda install -c conda-forge torchmetrics==0.9.2
$ conda install -c conda-forge packaging==21.0
$ conda install -c conda-forge seaborn==0.11.2
$ conda install -c conda-forge opentsne==0.6.2

# ì†ŒìŠ¤ ì½”ë“œ í´ë¡ 
$ git clone https://github.com/ChangwenXu98/TransPolymer.git
$ cd TransPolymer
```

> [!NOTE]
> **í˜‘ì—…ì ì°¸ê³  ì‚¬í•­:** PyTorch ë“±ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëŒ€ìš©ëŸ‰ ë°”ì´ë„ˆë¦¬ íŒŒì¼(`*.dll`, `*.lib` ë“±)ì€ ë ˆí¬ì§€í† ë¦¬ ê²½ëŸ‰í™”ë¥¼ ìœ„í•´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ì„¤ì¹˜ ë‹¨ê³„ë¥¼ ë”°ë¼ ë¡œì»¬ í™˜ê²½ì„ êµ¬ì¶•í•˜ë©´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í•¨ê»˜ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë©ë‹ˆë‹¤.


### ë°ì´í„°ì…‹

ì‚¬ì „ í•™ìŠµ(Pretraining) ë°ì´í„°ì…‹ì€ ["PI1M: A Benchmark Database for Polymer Informatics"](https://pubs.acs.org/doi/10.1021/acs.jcim.0c00726) ë…¼ë¬¸ì˜ ë°ì´í„°ë¥¼ ì±„íƒí–ˆìŠµë‹ˆë‹¤. ë°ì´í„° ì¦ê°•(Data augmentation)ì„ í†µí•´ ê° ì‹œë‚˜ë¦¬ì˜¤ë¥¼ 5ê°œë¡œ ëŠ˜ë ¸ìœ¼ë©°, ë” ì‘ì€ í¬ê¸°ì˜ ë°ì´í„°ì…‹ì€ PI1Mì—ì„œ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•˜ì—¬ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

í•˜ìœ„ ì‘ì—…(Downstream tasks)ì—ëŠ” ê³ ë¶„ì ì „í•´ì§ˆ ì „ë„ë„, ë°´ë“œ ê°­, ì „ì ì¹œí™”ë„, ì´ì˜¨í™” ì—ë„ˆì§€, ê²°ì •í™” ê²½í–¥, ìœ ì „ ìƒìˆ˜, êµ´ì ˆë¥ , pí˜• ê³ ë¶„ì OPV ì „ë ¥ ë³€í™˜ íš¨ìœ¨ ë“± 10ê°€ì§€ ë°ì´í„°ì…‹ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° ì²˜ë¦¬ ë° ì¦ê°•ì€ íŒŒì¸íŠœë‹ ë‹¨ê³„ ì „ì— ìˆ˜í–‰ë©ë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ì…‹ ì¶œì²˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

PE-I: ["AI-Assisted Exploration of Superionic Glass-Type Li(+) Conductors with Aromatic Structures"](https://pubs.acs.org/doi/10.1021/jacs.9b11442)

PE-II: ["Database Creation, Visualization, and Statistical Learning for Polymer Li+-Electrolyte Design"](https://pubs.acs.org/doi/full/10.1021/acs.chemmater.0c04767)

---

### ì¶”ê°€ëœ í•µì‹¬ ë°ì´í„°ì…‹
ë³¸ í”„ë¡œì íŠ¸ì˜ ë°œì „ì„ ìœ„í•´ ì¶”ê°€ëœ í•µì‹¬ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤:

1. **NeurIPS - Open Polymer Prediction 2025**
   - **ì¶œì²˜**: [Kaggle Competition](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025/data)
   - **ë¬¼ì„±**: Tg, FFV, Tc, Density, Rg (í†µí•© ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµì˜ ê¸°ë°˜)
   - **ì„¤ëª…**: MD ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ì˜ ê³ í’ˆì§ˆ ê³ ë¶„ì ë¬¼ì„± ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

2. **POINT2-Dataset (Polymer Property Tg)**
   - **ì¶œì²˜**: [Kaggle Dataset](https://www.kaggle.com/datasets/fridaycode/point2-dataset-polymer-property-tg-smiles), [GITHUB](https://github.com/Jiaxin-Xu/POINT2)
   - **ë¬¼ì„±**: ìœ ë¦¬ ì „ì´ ì˜¨ë„ (Tg) - ì•½ 7,210ê°œ ìƒ˜í”Œ
   - **ì„¤ëª…**: ì ‘ì°©ì œ ê°œë°œì˜ ì •ë°€ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ëŒ€ìš©ëŸ‰ Tg ì „ìš© ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

---

### í† í°í™” (Tokenization)
`PolymerSmilesTokenization.py`ëŠ” [huggingface](https://github.com/huggingface/transformers/tree/v4.21.2)ì˜ RobertaTokenizerë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, í™”í•™ êµ¬ì¡° ì¸ì‹ì„ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ì²´í¬í¬ì¸íŠ¸ (Checkpoints)
ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ `ckpt` í´ë”ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### [ì‹ ê·œ] í”„ë¡œì íŠ¸ êµ¬ì¡°
- **`app.py`**: í†µí•© 6ê°œ ë¬¼ì„± ì˜ˆì¸¡ Streamlit ëŒ€ì‹œë³´ë“œ.
- **`Downstream.py`**: ë©€í‹°íƒœìŠ¤í¬ ë° ë‹¨ì¼ íƒœìŠ¤í¬ íŒŒì¸íŠœë‹ í•µì‹¬ ë¡œì§.
- **`configs/`**: YAML ì„¤ì • íŒŒì¼ ì¤‘ì•™ ì €ì¥ì†Œ.
- **`utils/`**: ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ë„êµ¬ (t-SNE, Attention ë§µí•‘ ë“±).
- **`maintenance/`**: í™˜ê²½ ì„¤ì • ë° ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸.
- **`ckpt/` & `data/`**: ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ë°ì´í„°ì…‹ ì €ì¥ì†Œ.

## ëª¨ë¸ ì‹¤í–‰í•˜ê¸°

### ì‚¬ì „ í•™ìŠµ (Pretraining)
ì„¤ì • íŒŒì¼ì€ `configs/config.yaml`ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```bash
# MacOS / Single Process
$ python Pretrain.py --config configs/config.yaml

# Linux / Multi-GPU (Legacy)
$ python -m torch.distributed.launch --nproc_per_node=2 Pretrain.py --config configs/config.yaml
```
ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ *DistributedDataParallel*ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. í•™ìŠµ ê²°ê³¼ëŠ” `ckpt/pretrain.pt`ì— ì €ì¥ë©ë‹ˆë‹¤.

### íŒŒì¸íŠœë‹ (Finetuning)
ê¸°ë³¸ ì„¤ì • íŒŒì¼ì€ `configs/config_finetune.yaml`ì…ë‹ˆë‹¤.
```bash
$ python Downstream.py --config configs/config_finetune.yaml
```
ë©€í‹°íƒœìŠ¤í¬(5ê°œ ë¬¼ì„± í†µí•©) í•™ìŠµ ì‹œ:
```bash
$ python Downstream.py --config configs/config_finetune_Multi.yaml
```

## ì‹œê°í™”

### Attention ì‹œê°í™”
í•´ì„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ Attention ìŠ¤ì½”ì–´ ì‹œê°í™” ì„¤ì •ì€ `configs/config_attention.yaml`ì— ìˆìŠµë‹ˆë‹¤.
```bash
$ python utils/Attention_vis.py --config configs/config_attention.yaml
```

### t-SNE ì‹œê°í™”
í™”í•™ì  ê³µê°„ ë¶„í¬ ì‹œê°í™” ì„¤ì •ì€ `configs/config_tSNE.yaml`ì— ìˆìŠµë‹ˆë‹¤.
```bash
$ python utils/tSNE.py --config configs/config_tSNE.yaml
```

---

## ê³ ë¶„ì ë¬¼ì„± í†µí•© ëŒ€ì‹œë³´ë“œ (Streamlit UI)

ë³¸ ë ˆí¬ì§€í† ë¦¬ì˜ í•µì‹¬ ê¸°ëŠ¥ì¸ **í†µí•© 6ì¢… ë¬¼ì„± ì˜ˆì¸¡ ì‹œìŠ¤í…œ**ì€ ì‚¬ìš©ìê°€ ë³„ë„ì˜ ì½”ë”© ì—†ì´ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ê³ ë¶„ìì˜ ì„±ì§ˆì„ ì¦‰ì‹œ ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- **ë™ì‹œ ì˜ˆì¸¡**: í•˜ë‚˜ì˜ SMILES ì…ë ¥ìœ¼ë¡œ 6ê°€ì§€ í•µì‹¬ ë¬¼ì„±(Tg, FFV, Tc, Density, Rg, Conductivity)ì„ í•œ ë²ˆì— ì˜ˆì¸¡í•©ë‹ˆë‹¤.
- **Attention ì‹œê°í™”**: ì¸ê³µì§€ëŠ¥ì´ í™”í•™ êµ¬ì¡°ì˜ ì–´ëŠ ë¶€ë¶„ì— ì£¼ëª©í•˜ì—¬ ìˆ˜ì¹˜ë¥¼ ê³„ì‚°í–ˆëŠ”ì§€ ì¸í„°ë™í‹°ë¸Œ íˆíŠ¸ë§µìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ëª¨ë‘ ì§€ì›í•˜ë©°, ë¬¼ë¦¬ ë‹¨ìœ„ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### ì‹¤í–‰ ë°©ë²•
1. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:
   ```bash
   $ streamlit run app.py
1.  í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:
    ```bash
    $ streamlit run app.py
    ```
2.  ì›¹ ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©´ SMILES ì…ë ¥ì¹¸ì— ë¶„ì„í•˜ë ¤ëŠ” ê³ ë¶„ìì˜ êµ¬ì¡°ì‹ì„ ì…ë ¥í•˜ê³  "ëª¨ë“  ë¬¼ì„± ì˜ˆì¸¡í•˜ê¸°" ë²„íŠ¼ì„ ëˆ„ë¦…ë‹ˆë‹¤.

### ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¬¼ì„± ëª©ë¡
| ë¬¼ì„±ëª… (KR) | ì˜ë¬¸ëª… & ê¸°í˜¸ | ë‹¨ìœ„ |
| :--- | :--- | :--- |
| **ìœ ë¦¬ ì „ì´ ì˜¨ë„** | Glass Transition (Tg) | Â°C | (v1.1 Boost ì ìš©) |
| **ììœ  ë¶€í”¼ë¹„** | Free Volume (FFV) | - |
| **ì—´ì „ë„ë„** | Thermal Cond (Tc) | W/mK |
| **ë°€ë„** | Density | g/cmÂ³ |
| **íšŒì „ ë°˜ê²½** | Radius of Gyration (Rg) | Ã… |
| **ì´ì˜¨ ì „ë„ë„** | Ionic Conductivity | S/cm |

---

## ì ‘ì°©ì œ/ì ì°©ì œ íŠ¹í™” Tg-Boost ëª¨ë¸

ì‚°ì—…ê³„(Adhesive/PSA) ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•˜ì—¬, **POINT2 ë°ì´í„°ì…‹(7,210ê°œ)**ì„ í™œìš©í•œ ê³ ì •ë°€ Tg ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ í™˜ê²½ì„ êµ¬ì¶•í•´ ë‘ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ í•™ìˆ  ë°ì´í„°ì…‹ë³´ë‹¤ ì•½ 10ë°° ë§ì€ ì–‘ì„ í•™ìŠµí•˜ì—¬ í›¨ì”¬ ì •ë°€í•œ êµ¬ì¡°-ë¬¼ì„± ìƒê´€ê´€ê³„ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.

### Tg-Boost í•™ìŠµ ì‹¤í–‰
ì œê³µëœ ë°°ì¹˜ íŒŒì¼ì„ í†µí•´ ê°„í¸í•˜ê²Œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```bash
$ .\run_tg_boost.bat
```
*(ê²°ê³¼ë¬¼ì€ `ckpt/model_multi_boost_best.pt`ë¡œ ì €ì¥ë©ë‹ˆë‹¤.)*

---

## English Version (Original) ##

## TransPolymer ##

#### npj Computational Materials [[Paper]](https://www.nature.com/articles/s41524-023-01016-5) [[arXiv]](https://arxiv.org/abs/2209.01307) [[PDF]](https://www.nature.com/articles/s41524-023-01016-5.pdf) </br>
[Changwen Xu](https://changwenxu98.github.io/), [Yuyang Wang](https://yuyangw.github.io/), [Amir Barati Farimani](https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html) </br>
Carnegie Mellon University </br>

<img src="figs/pipeline.png" width="500">

This is the official implementation of <strong><em>TransPolymer</em></strong>: ["TransPolymer: a Transformer-based language model for polymer property predictions"](https://www.nature.com/articles/s41524-023-01016-5). In this work, we introduce TransPolymer, a Transformer-based language model, for representation learning of polymer sequences by pretraining on a large unlabeled dataset (~5M polymer sequences) via self-supervised masked language modeling and making accurate and efficient predictions of polymer properties in downstream tasks by finetuning. If you find our work useful in your research, please cite:
```
@article{xu2023transpolymer,
  title={TransPolymer: a Transformer-based language model for polymer property predictions},
  author={Xu, Changwen and Wang, Yuyang and Barati Farimani, Amir},
  journal={npj Computational Materials},
  volume={9},
  number={1},
  pages={64},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
```

## Getting Started

### ğŸ”¥ Quick Start (MacOS/Apple Silicon Demo)

For a quick setup on a clean MacBook (Apple Silicon), use the provided **one-click setup script**.
Run the following command in your terminal to automatically create a virtual environment and install all dependencies.

```bash
bash setup_mac.sh
```

After setup:
```bash
# Activate virtual environment
source .venv/bin/activate

# Run Demo App
streamlit run app.py

# Or Run Training Demo
python Pretrain.py --config configs/config.yaml

# Run Finetuning
bash run_finetune.sh

# Run Tg-Boost
bash run_tg_boost.sh
```

### Installation

Set up conda environment and clone the github repo

```
# create a new environment
$ conda create --name TransPolymer python=3.9
$ conda activate TransPolymer

# install requirements
$ conda install pytorch==1.12.0 torchvision torchaudio cudatoolkit=11.3 -c pytorch
$ pip install transformers==4.20.1
$ pip install PyYAML==6.0
$ pip install fairscale==0.4.6
$ conda install -c conda-forge rdkit=2022.3.4
$ conda install -c conda-forge scikit-learn==0.24.2
$ conda install -c conda-forge tensorboard==2.9.1
$ conda install -c conda-forge torchmetrics==0.9.2
$ conda install -c conda-forge packaging==21.0
$ conda install -c conda-forge seaborn==0.11.2
$ conda install -c conda-forge opentsne==0.6.2

# clone the source code of TransPolymer
$ git clone https://github.com/ChangwenXu98/TransPolymer.git
$ cd TransPolymer
```

> [!NOTE]
> **For Collaborators:** Large binary files (e.g., `*.dll`, `*.lib` from PyTorch) are excluded from the repository to keep it lightweight. Please follow the installation steps above to set up your local environment. These files will be automatically installed with the required libraries.


### Dataset

The pretraining dataset is adopted from the paper ["PI1M: A Benchmark Database for Polymer Informatics"](https://pubs.acs.org/doi/10.1021/acs.jcim.0c00726). Data augmentation is applied by augmenting each sequence to five. Pretraining data with smaller sizes are obtained by randomly picking up data entries from PI1M dataset.

Ten datasets, concerning different polymer properties including polymer electrolyte conductivity, band gap, electron affinity, ionization energy, crystallization tendency, dielectric constant, refractive index, and p-type polymer OPV power conversion efficiency, are used for downstream tasks. Data processing and augmentation are implemented before usage in the finetuning stage. The original datasets and their sources are listed below:

PE-I: ["AI-Assisted Exploration of Superionic Glass-Type Li(+) Conductors with Aromatic Structures"](https://pubs.acs.org/doi/10.1021/jacs.9b11442)

PE-II: ["Database Creation, Visualization, and Statistical Learning for Polymer Li+-Electrolyte Design"](https://pubs.acs.org/doi/full/10.1021/acs.chemmater.0c04767)

Egc, Egb, Eea, Ei, Xc, EPS, Nc: ["Polymer informatics with multi-task learning"](https://www.sciencedirect.com/science/article/pii/S2666389921000581)

---

### [User-Provided Primary Datasets]
Key datasets provided by the user for project enhancement:

1. **NeurIPS - Open Polymer Prediction 2025**
   - **Source**: [Kaggle Competition](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025/data)
   - **Properties**: Tg, FFV, Tc, Density, Rg (Core Multi-Task data)
2. **POINT2-Dataset (Polymer Property Tg)**
   - **Source**: [Kaggle Dataset](https://www.kaggle.com/datasets/fridaycode/point2-dataset-polymer-property-tg-smiles), [GITHUB](https://github.com/Jiaxin-Xu/POINT2)
   - **Properties**: Glass Transition Temperature (Tg) - ~7,210 samples

---

### Tokenization
`PolymerSmilesTokenization.py` is adapted from RobertaTokenizer from [huggingface](https://github.com/huggingface/transformers/tree/v4.21.2) with a specially designed regular expression for tokenization with chemical awareness.

### Checkpoints
Pretrained model can be found in `ckpt` folder.

### [NEW] Project Structure
- **`app.py`**: Unified 6-Property Streamlit Dashboard.
- **`Downstream.py`**: Core logic for multi-task and single-task fine-tuning.
- **`configs/`**: Centralized storage for YAML configurations.
- **`utils/`**: Data processing and analytical tools (t-SNE, Attention mapping).
- **`maintenance/`**: Environment setup and diagnostic scripts.
- **`ckpt/` & `data/`**: Model weights and dataset storage.

## Run the Model

### Pretraining
To pretrain TransPolymer, the configuration can be found in `configs/config.yaml`.
```bash
$ python -m torch.distributed.launch --nproc_per_node=2 Pretrain.py --config configs/config.yaml
```
*DistributedDataParallel* is used for faster pretraining. The pretrained model can be found in `ckpt/pretrain.pt`

### Finetuning
To finetune the pretrained TransPolymer, find configurations in `configs/config_finetune.yaml`.
```bash
$ python Downstream.py --config configs/config_finetune.yaml
```
For Multi-task (5 properties):
```bash
$ python Downstream.py --config configs/config_finetune_Multi.yaml
```

### Attention Visualization
To visualize attention scores, configurations are in `configs/config_attention.yaml`.
```bash
$ python utils/Attention_vis.py --config configs/config_attention.yaml
```

### t-SNE Visualization
To visualize the chemical space, configurations are in `configs/config_tSNE.yaml`.
```bash
$ python utils/tSNE.py --config configs/config_tSNE.yaml
```

## Acknowledgement
- PyTorch implementation of Transformer: [https://github.com/huggingface/transformers.git](https://github.com/huggingface/transformers.git)
